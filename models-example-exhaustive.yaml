## `instruct` models configuration file
## 1. Follow the model installation instructions or get the API key from the respective model provider
## 2. Uncomment and fill in the values for the models you want to use
## See the documentation for more details: https://github.com/xbasset/instruct

## For local models, you can use the following configuration
## Pre-requisite: Ollama installation instructions https://ollama.com/
## For cloud models, create an account and get the API key from the respective model provider

models:
  ollama/llama3.1:
      name: llama3.1
      base_url: "http://localhost:11434"
  ollama/llama3:
      name: llama3
      base_url: "http://localhost:11434"
  ollama/gemma2:
      name: gemma2
      base_url: "http://localhost:11434"
  ollama/mistral-nemo:
      name: mistral-nemo
      base_url: "http://localhost:11434"

  openai/gpt-4-turbo-preview:
      name: gpt-4-turbo
      api_key: <your-api-key>
  openai/gpt-4o:
      name: gpt-4o
      api_key: <your-api-key>

  azure/prod-gpt4o:
      client: openai
      name: gpt-4o
      base_url: <your-endpoint>
      api_version: <your-api-version>
      api_key: <your-api-key>

  azure/mistral-large-latest:
      client: mistral
      name: mistral-large
      base_url: <your-endpoint>
      api_key: <your-api-key>

  mistral/mistral-large-2407:
      name: mistral-large
      api_key: <your-api-key>

  groq/llama3-70b-8192:
      name: llama3-70b-8192
      api_key: <your-api-key>
  groq/mixtral-8x7b-32768:
      name: mixtral-8x7b-32768
      api_key: <your-api-key>