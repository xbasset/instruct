## `instruct` models configuration file
## 1. Follow the model installation instructions or get the API key from the respective model provider
## 2. Uncomment and fill in the values for the models you want to use
## See the documentation for more details: https://github.com/xbasset/instruct
## If you don't find your preferred provider / model, please open an issue on GitHub

## For local models, you can use the following configuration
## Pre-requisite: Ollama installation instructions https://ollama.com/

# ollama:
#   llm:
#     mistral:
#       endpoint: "http://localhost:11434"
#     llama3:
#       endpoint: "http://localhost:11434"
#     phi3:
#       endpoint: "http://localhost:11434"
#     gemma2:
#       endpoint: "http://localhost:11434"
#     mistral-nemo:
#       endpoint: "http://localhost:11434"
  

## For cloud models, create an account and get the API key from the respective model provider

openai:
  llm:
    gpt-4-turbo-preview:
      api_key: <API_KEY>
    gpt-4o:
      api_key: <API_KEY>

azure:
  llm:
    gpt4-turbo:
      client: openai
      api_type: azure
      endpoint: <ENDPOINT>
      api_version: <API_VERSION>
      api_key: <API_KEY>
      deployment_id: <DEPLOYMENT_ID>
    gpt-4o:
      client: openai
      api_type: azure
      endpoint: <ENDPOINT>
      api_version: <API_VERSION>
      api_key: <API_KEY>
      deployment_id: <DEPLOYMENT_ID>
    mistral-large-latest:
      client: mistral
      api_type: azure
      api_base: <API_BASE>
      api_key: <API_KEY>
  vision:
    gpt-4o-vision:
      client: openai
      api_type: azure
      endpoint: <ENDPOINT>
      api_version: <API_VERSION>
      api_key: <API_KEY>
      deployment_id: <DEPLOYMENT_ID>



mistral:
  llm:
    mistral-large-2402:
      api_key: <API_KEY>
    mistral-large-2407:
      api_key: <API_KEY>
    mistral-large-latest:
      api_key: <API_KEY>

groq:
  llm:
    llama3-70b-8192:
      api_key: <API_KEY>
    mixtral-8x7b-32768:
      api_key: <API_KEY>