## `instruct` models configuration file
## 1. Follow the model installation instructions or get the API key from the respective model provider
## 2. Uncomment and fill in the values for the models you want to use
## See the documentation for more details: https://github.com/xbasset/instruct
## If you don't find your preferred provider / model, please open an issue on GitHub

## For local models, you can use the following configuration
## Pre-requisite: Ollama installation instructions https://ollama.com/

# ollama:
#   mistral:
#     endpoint: "http://localhost:11434"
#   llama3:
#     endpoint: "http://localhost:11434"

## For cloud models, create an account and get the API key from the respective model provider

# openai:
#   gpt-4o:
#     api_key: <your-openai-api-key>
#   gpt-4-turbo-preview:
#     api_key: <your-openai-api-key>


# azure:
#   gpt-4-turbo:
#     gpt4_turbo_azure_openai_api_base: <your-azure-openai-api-base>
#     gpt4_turbo_azure_openai_api_version: <your-azure-openai-api-version>
#     gpt4_turbo_azure_openai_key: <your-azure-openai-key>
#     gpt4_turbo_azure_openai_deployment_id: <your-azure-openai-deployment-id>

# mistral:
#   mistral-large-2402:
#     mistral_api_key: <your-mistral-api-key>

# groq:
#   llama3-70b-8192:
#     api_key: <your-groq-api-key>
#   mixtral-8x7b-32768:
#     api_key: <your-groq-api-key>